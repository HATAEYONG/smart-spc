"""
Time Series Analysis Service
ì‹œê³„ì—´ ë¶„ì„ ë° ì˜ˆì¸¡ ê¸°ëŠ¥ (ARIMA, Prophet ìŠ¤íƒ€ì¼)
"""
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime, timedelta
from collections import deque

from django.db.models import Avg, StdDev, Count
from apps.spc.models import QualityMeasurement, Product


class TimeSeriesAnalyzer:
    """ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ê¸°"""

    def __init__(self):
        self.min_samples = 10  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜

    def analyze_trend(
        self,
        measurements: List[float],
        timestamps: List[datetime]
    ) -> Dict[str, Any]:
        """
        ì¶”ì„¸ ë¶„ì„ (Trend Analysis)

        Args:
            measurements: ì¸¡ì •ê°’ ë¦¬ìŠ¤íŠ¸
            timestamps: íƒ€ì„ìŠ¤íƒ¬í”„ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì¶”ì„¸ ë¶„ì„ ê²°ê³¼
        """
        if len(measurements) < self.min_samples:
            return {
                'trend': 'unknown',
                'slope': 0,
                'correlation': 0,
                'interpretation': 'ë°ì´í„° ë¶€ì¡±',
            }

        # ì„ í˜• íšŒê·€ë¡œ ì¶”ì„¸ ê³„ì‚°
        x = np.arange(len(measurements))
        y = np.array(measurements)

        # 1ì°¨ ë‹¤í•­ì‹ ì í•©
        coefficients = np.polyfit(x, y, 1)
        slope = coefficients[0]
        intercept = coefficients[1]

        # ìƒê´€ê³„ìˆ˜ (RÂ²)
        y_pred = np.polyval(coefficients, x)
        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)
        r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0

        # ì¶”ì„¸ íŒì •
        if abs(slope) < 0.001:
            trend = "stable"
            interpretation = "ì•ˆì •ì ì¸ ì¶”ì„¸"
        elif slope > 0:
            trend = "increasing"
            if r_squared > 0.7:
                interpretation = f"ìƒìŠ¹ ì¶”ì„¸ (ê¸°ìš¸ê¸°: {slope:.6f}, ê°•ë„: ê°•í•¨)"
            else:
                interpretation = f"ì•½ê°„ ìƒìŠ¹ ì¶”ì„¸ (ê¸°ìš¸ê¸°: {slope:.6f})"
        else:
            trend = "decreasing"
            if r_squared > 0.7:
                interpretation = f"í•˜ë½ ì¶”ì„¸ (ê¸°ìš¸ê¸°: {slope:.6f}, ê°•ë„: ê°•í•¨)"
            else:
                interpretation = f"ì•½ê°„ í•˜ë½ ì¶”ì„¸ (ê¸°ìš¸ê¸°: {slope:.6f})"

        return {
            'trend': trend,
            'slope': float(slope),
            'intercept': float(intercept),
            'r_squared': float(r_squared),
            'correlation': float(np.sqrt(r_squared)),
            'interpretation': interpretation,
        }

    def detect_seasonality(
        self,
        measurements: List[float],
        period: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        ê³„ì ˆì„± ê²€ì¶œ (Seasonality Detection)

        Args:
            measurements: ì¸¡ì •ê°’ ë¦¬ìŠ¤íŠ¸
            period: ì˜ˆìƒ ì£¼ê¸° (Noneì´ë©´ ìë™ ê°ì§€)

        Returns:
            ê³„ì ˆì„± ë¶„ì„ ê²°ê³¼
        """
        if len(measurements) < 20:
            return {
                'has_seasonality': False,
                'period': None,
                'strength': 0,
                'interpretation': 'ë°ì´í„° ë¶€ì¡±',
            }

        measurements_array = np.array(measurements)
        n = len(measurements_array)

        # ìë™ ì£¼ê¸° ê°ì§€ (FFT ê¸°ë°˜)
        if period is None:
            fft = np.fft.fft(measurements_array - np.mean(measurements_array))
            freqs = np.fft.fftfreq(n)
            power = np.abs(fft) ** 2

            # ì£¼íŒŒìˆ˜ ì œì™¸ (DC ì„±ë¶„)
            power[0] = 0

            # ê°€ì¥ ê°•í•œ ì£¼íŒŒìˆ˜ ì„ íƒ
            dominant_freq_idx = np.argmax(power)
            dominant_freq = freqs[dominant_freq_idx]

            if dominant_freq > 0:
                period = int(1 / dominant_freq)
                period = min(period, n // 2)  # ë„ˆë¬´ ê¸´ ì£¼ê¸° ë°©ì§€
            else:
                period = None

        if period is None or period <= 1:
            return {
                'has_seasonality': False,
                'period': None,
                'strength': 0,
                'interpretation': 'ê³„ì ˆì„± ì—†ìŒ',
            }

        # ê³„ì ˆì„± ê°•ë„ ê³„ì‚° (ë¶„ì‚° ë¶„ì„)
        if period > 0 and period < n:
            # ì£¼ê¸°ë³„ í‰ê·  ê³„ì‚°
            seasonal_means = []
            for i in range(period):
                indices = range(i, n, period)
                period_data = measurements_array[indices]
                if len(period_data) > 0:
                    seasonal_means.append(np.mean(period_data))

            overall_mean = np.mean(measurements_array)
            seasonal_variance = np.var(seasonal_means) if len(seasonal_means) > 1 else 0
            total_variance = np.var(measurements_array)

            strength = seasonal_variance / total_variance if total_variance > 0 else 0
        else:
            strength = 0

        has_seasonality = strength > 0.1

        return {
            'has_seasonality': has_seasonality,
            'period': period if has_seasonality else None,
            'strength': float(strength),
            'interpretation': f"ì£¼ê¸° {period} ê³„ì ˆì„± ë°œê²¬" if has_seasonality else "ê³„ì ˆì„± ì—†ìŒ",
        }

    def decompose(
        self,
        measurements: List[float]
    ) -> Dict[str, Any]:
        """
        ì‹œê³„ì—´ ë¶„í•´ (Trend + Seasonal + Residual)

        Args:
            measurements: ì¸¡ì •ê°’ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë¶„í•´ëœ ì„±ë¶„ë“¤
        """
        measurements_array = np.array(measurements)
        n = len(measurements_array)

        # ì´ë™í‰ê·  (ì¶”ì„¸)
        window_size = min(7, n // 4)
        if window_size < 3:
            window_size = 3

        trend = np.convolve(
            measurements_array,
            np.ones(window_size) / window_size,
            mode='same'
        )

        # ê³„ì ˆì„± (ì¶”ì„¸ ì œê±° í›„)
        detrended = measurements_array - trend
        seasonal = np.zeros_like(detrended)

        period = 7  # ì¼ì£¼ì¼ ì£¼ê¸° ê°€ì •
        if n >= period * 2:
            for i in range(period):
                indices = range(i, n, period)
                period_mean = np.mean(detrended[list(indices)])
                for j in indices:
                    if j < n:
                        seasonal[j] = period_mean

        # ì”ì°¨ (Residual)
        residual = measurements_array - trend - seasonal

        return {
            'trend': trend.tolist(),
            'seasonal': seasonal.tolist(),
            'residual': residual.tolist(),
            'trend_strength': float(np.var(trend) / np.var(measurements_array)) if np.var(measurements_array) > 0 else 0,
            'seasonal_strength': float(np.var(seasonal) / np.var(measurements_array)) if np.var(measurements_array) > 0 else 0,
            'residual_strength': float(np.var(residual) / np.var(measurements_array)) if np.var(measurements_array) > 0 else 0,
        }


class ForecastEngine:
    """ì˜ˆì¸¡ ì—”ì§„ (ARIMA ë‹¨ìˆœí™” ë²„ì „)"""

    def __init__(self):
        self.analyzer = TimeSeriesAnalyzer()

    def simple_ma_forecast(
        self,
        measurements: List[float],
        forecast_steps: int = 5,
        window_size: int = 7
    ) -> Dict[str, Any]:
        """
        ë‹¨ìˆœ ì´ë™í‰ê·  ì˜ˆì¸¡ (Simple Moving Average)

        Args:
            measurements: ê³¼ê±° ì¸¡ì •ê°’
            forecast_steps: ì˜ˆì¸¡í•  ë¯¸ë˜ ê°œìˆ˜
            window_size: ì´ë™í‰ê·  ìœˆë„ìš°

        Returns:
            ì˜ˆì¸¡ ê²°ê³¼
        """
        if len(measurements) < window_size:
            window_size = len(measurements)

        # ìµœê·¼ window_sizeê°œì˜ í‰ê· ìœ¼ë¡œ ì˜ˆì¸¡
        last_values = measurements[-window_size:]
        forecast_value = np.mean(last_values)

        # ì‹ ë¢™ êµ¬ê°„ ì˜ˆì¸¡
        forecast = [forecast_value] * forecast_steps

        # í‘œì¤€í¸ì°¨ (ë¶ˆë¡œ ì˜ˆì¸¡ êµ¬ê°„)
        std = np.std(last_values)
        upper_bound = [forecast_value + 1.96 * std] * forecast_steps
        lower_bound = [forecast_value - 1.96 * std] * forecast_steps

        return {
            'method': 'Simple Moving Average',
            'forecast': forecast,
            'upper_bound': upper_bound,
            'lower_bound': lower_bound,
            'confidence': 0.95,
            'window_size': window_size,
        }

    def exponential_smoothing_forecast(
        self,
        measurements: List[float],
        forecast_steps: int = 5,
        alpha: float = 0.3
    ) -> Dict[str, Any]:
        """
        ì§€ìˆ˜í‰í™œë²• (Exponential Smoothing)

        Args:
            measurements: ê³¼ê±° ì¸¡ì •ê°’
            forecast_steps: ì˜ˆì¸¡í•  ë¯¸ë˜ ê°œìˆ˜
            alpha: í‰í™œ ê³„ìˆ˜ (0-1)

        Returns:
            ì˜ˆì¸¡ ê²°ê³¼
        """
        # ì´ˆê¸°ê°’ (ì´ˆê¸° 3ê°œì˜ í‰ê· )
        level = np.mean(measurements[:3])

        # ê´€ì¸¡ê°’ì— ëŒ€í•œ level ì—…ë°ì´íŠ¸
        for value in measurements:
            level = alpha * value + (1 - alpha) * level

        # ì˜ˆì¸¡
        forecast = [level] * forecast_steps

        # ì˜ˆì¸¡ ì˜¤ì°¨ ì¶”ì •
        errors = [abs(measurements[i] - measurements[i-1]) for i in range(1, len(measurements))]
        mad = np.mean(errors)  # Mean Absolute Deviation
        std_estimate = mad * 1.25  # MAD â†’ std ë³€í™˜

        upper_bound = [level + 1.96 * std_estimate] * forecast_steps
        lower_bound = [level - 1.96 * std_estimate] * forecast_steps

        return {
            'method': 'Exponential Smoothing',
            'forecast': forecast,
            'upper_bound': upper_bound,
            'lower_bound': lower_bound,
            'confidence': 0.95,
            'alpha': alpha,
            'final_level': float(level),
        }

    def linear_trend_forecast(
        self,
        measurements: List[float],
        forecast_steps: int = 5
    ) -> Dict[str, Any]:
        """
        ì„ í˜• ì¶”ì„¸ ì˜ˆì¸¡ (Linear Trend Forecast)

        Args:
            measurements: ê³¼ê±° ì¸¡ì •ê°’
            forecast_steps: ì˜ˆì¸¡í•  ë¯¸ë˜ ê°œìˆ˜

        Returns:
            ì˜ˆì¸¡ ê²°ê³¼
        """
        x = np.arange(len(measurements))
        y = np.array(measurements)

        # ì„ í˜• íšŒê·€
        coefficients = np.polyfit(x, y, 1)
        slope, intercept = coefficients

        # ì˜ˆì¸¡
        last_x = len(measurements) - 1
        forecast_x = np.arange(last_x + 1, last_x + 1 + forecast_steps)
        forecast = np.polyval(coefficients, forecast_x)

        # ì”ì°¨ í‘œì¤€í¸ì°¨
        y_pred = np.polyval(coefficients, x)
        residuals = y - y_pred
        std = np.std(residuals)

        # ì˜ˆì¸¡ êµ¬ê°„
        upper_bound = forecast + 1.96 * std
        lower_bound = forecast - 1.96 * std

        return {
            'method': 'Linear Trend',
            'forecast': forecast.tolist(),
            'upper_bound': upper_bound.tolist(),
            'lower_bound': lower_bound.tolist(),
            'confidence': 0.95,
            'slope': float(slope),
            'intercept': float(intercept),
            'r_squared': float(1 - np.var(residuals) / np.var(y)),
        }

    def combined_forecast(
        self,
        measurements: List[float],
        forecast_steps: int = 5
    ) -> Dict[str, Any]:
        """
        ê²°í•© ì˜ˆì¸¡ (ì•™ìƒë¸”)

        Args:
            measurements: ê³¼ê±° ì¸¡ì •ê°’
            forecast_steps: ì˜ˆì¸¡í•  ë¯¸ë˜ ê°œìˆ˜

        Returns:
            ê²°í•© ì˜ˆì¸¡ ê²°ê³¼
        """
        # ì„¸ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì˜ˆì¸¡
        ma_result = self.simple_ma_forecast(measurements, forecast_steps)
        es_result = self.exponential_smoothing_forecast(measurements, forecast_steps)
        lt_result = self.linear_trend_forecast(measurements, forecast_steps)

        # ì•™ìƒë¸” (ë‹¨ìˆœ í‰ê· )
        ma_forecast = np.array(ma_result['forecast'])
        es_forecast = np.array(es_result['forecast'])
        lt_forecast = np.array(lt_result['forecast'])

        combined_forecast = (ma_forecast + es_forecast + lt_forecast) / 3

        # ê²°í•© ì‹ ë¢° êµ¬ê°„
        ma_upper = np.array(ma_result['upper_bound'])
        es_upper = np.array(es_result['upper_bound'])
        lt_upper = np.array(lt_result['upper_bound'])

        ma_lower = np.array(ma_result['lower_bound'])
        es_lower = np.array(es_result['lower_bound'])
        lt_lower = np.array(lt_result['lower_bound'])

        combined_upper = (ma_upper + es_upper + lt_upper) / 3
        combined_lower = (ma_lower + es_lower + lt_lower) / 3

        return {
            'method': 'Combined (Ensemble)',
            'forecast': combined_forecast.tolist(),
            'upper_bound': combined_upper.tolist(),
            'lower_bound': combined_lower.tolist(),
            'confidence': 0.95,
            'components': {
                'moving_average': ma_result['forecast'],
                'exponential_smoothing': es_result['forecast'],
                'linear_trend': lt_result['forecast'],
            },
        }


class AnomalyDetector:
    """ì´ìƒ ê°ì§€ (Anomaly Detection)"""

    def __init__(self, threshold: float = 3.0):
        """
        Args:
            threshold: Z-score ì„ê³„ê°’ (ê¸°ë³¸ 3Ïƒ)
        """
        self.threshold = threshold

    def detect_statistical_anomalies(
        self,
        measurements: List[QualityMeasurement]
    ) -> List[Dict[str, Any]]:
        """
        í†µê³„ì  ì´ìƒ ê°ì§€ (Z-score ê¸°ë°˜)

        Args:
            measurements: QualityMeasurement ê°ì²´ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì´ìƒ ê°ì§€ëœ í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        anomalies = []
        values = [m.measurement_value for m in measurements]

        if len(values) < 5:
            return []

        # Z-score ê³„ì‚°
        mean = np.mean(values)
        std = np.std(values)

        if std == 0:
            return []

        for i, measurement in enumerate(measurements):
            z_score = abs((measurement.measurement_value - mean) / std)

            if z_score > self.threshold:
                anomalies.append({
                    'index': i,
                    'measurement_id': measurement.id,
                    'value': float(measurement.measurement_value),
                    'z_score': float(z_score),
                    'timestamp': measurement.measured_at.isoformat(),
                    'type': 'statistical_outlier',
                    'severity': 'high' if z_score > 5 else 'medium',
                })

        return anomalies

    def detect_pattern_anomalies(
        self,
        measurements: List[QualityMeasurement]
    ) -> List[Dict[str, Any]]:
        """
        íŒ¨í„´ ê¸°ë°˜ ì´ìƒ ê°ì§€

        - ê¸‰ê²©í•œ ë³€í™” (Spike)
        - ë°©í–¥ ë³€í™” (Trend Change)
        - ì´ìƒ íŒ¨í„´ (Cyclic/Periodic)

        Args:
            measurements: QualityMeasurement ê°ì²´ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì´ìƒ ê°ì§€ëœ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸
        """
        anomalies = []
        values = [m.measurement_value for m in measurements]

        if len(values) < 10:
            return []

        # 1. ê¸‰ê²©í•œ ë³€í™” (Spike) ê°ì§€
        mean = np.mean(values)
        std = np.std(values)

        for i in range(1, len(values)):
            delta = abs(values[i] - values[i-1])

            # ì´ì „ ê°’ê³¼ì˜ ì°¨ì´ê°€ 3Ïƒ ì´ìƒì¸ ê²½ìš°
            if delta > 3 * std:
                anomalies.append({
                    'index': i,
                    'type': 'spike',
                    'value': float(values[i]),
                    'previous_value': float(values[i-1]),
                    'delta': float(delta),
                    'timestamp': measurements[i].measured_at.isoformat(),
                    'severity': 'high' if delta > 5 * std else 'medium',
                })

        # 2. ë°©í–¥ ë³€í™” ê°ì§€ (Cumulative Sum)
        if len(values) >= 20:
            half = len(values) // 2
            first_half_mean = np.mean(values[:half])
            second_half_mean = np.mean(values[half:])

            # ì „ë°˜ë¶€ì™€ í›„ë°˜ë¶€ì˜ í‰ê·  ì°¨ì´ê°€ í° ê²½ìš°
            if abs(second_half_mean - first_half_mean) > 2 * std:
                anomalies.append({
                    'type': 'trend_shift',
                    'first_half_mean': float(first_half_mean),
                    'second_half_mean': float(second_half_mean),
                    'shift': float(second_half_mean - first_half_mean),
                    'severity': 'medium',
                })

        return anomalies

    def calculate_anomaly_score(
        self,
        measurement: QualityMeasurement,
        historical_values: List[float]
    ) -> float:
        """
        ë‹¨ì¼ ì¸¡ì •ê°’ì˜ ì´ìƒ ì ìˆ˜ ê³„ì‚°

        0-100 ì‚¬ì´ì˜ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì •ìƒ)

        Args:
            measurement: í‰ê°€í•  ì¸¡ì •ê°’
            historical_values: ê³¼ê±° ì¸¡ì •ê°’ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì´ìƒ ì ìˆ˜ (0-100)
        """
        if len(historical_values) < 5:
            return 100  # ë°ì´í„° ë¶€ì¡±ì‹œ ì •ìƒìœ¼ë¡œ ê°„ì£¼

        mean = np.mean(historical_values)
        std = np.std(historical_values)

        if std == 0:
            return 100

        # Z-score ê¸°ë°˜ ì ìˆ˜
        z_score = abs((measurement.measurement_value - mean) / std)

        # Z-scoreë¥¼ 0-100 ì ìˆ˜ë¡œ ë³€í™˜
        # Z-scoreê°€ 0ì´ë©´ 100ì , 3ì´ë©´ 0ì 
        score = max(0, 100 - (z_score * 33.33))

        return float(score)


class PredictiveMaintenance:
    """ì˜ˆì¸¡ ìœ ì§€ë³´ìˆ˜ (Predictive Maintenance)"""

    def calculate_equipment_health(
        self,
        measurements: List[QualityMeasurement],
        target_value: float,
        tolerance: float
    ) -> Dict[str, Any]:
        """
        ì„¤ë¹„ ê±´ì „ì„± ì§€ìˆ˜ ê³„ì‚°

        Args:
            measurements: ìµœê·¼ ì¸¡ì •ê°’ë“¤
            target_value: ëª©í‘œê°’
            tolerance: í—ˆìš© ì˜¤ì°¨

        Returns:
            ê±´ì „ì„± ë¶„ì„ ê²°ê³¼
        """
        if not measurements:
            return {
                'health_score': 0,
                'status': 'unknown',
                'interpretation': 'ë°ì´í„° ì—†ìŒ',
            }

        values = [m.measurement_value for m in measurements]
        mean = np.mean(values)
        std = np.std(values)

        # ëª©í‘œê°’ì—ì„œì˜ í¸ì°¨
        deviation = abs(mean - target_value)

        # ê±´ì „ì„± ì ìˆ˜ (0-100)
        # í¸ì°¨ê°€ 0ì´ë©´ 100ì , tolerance ì´ìƒì´ë©´ 0ì 
        if tolerance > 0:
            health_score = max(0, 100 * (1 - deviation / tolerance))
        else:
            health_score = 100 if deviation == 0 else 0

        # ìƒíƒœ íŒì •
        if health_score >= 90:
            status = 'excellent'
            interpretation = 'ì–‘í˜¸'
        elif health_score >= 70:
            status = 'good'
            interpretation = 'ì •ìƒ'
        elif health_score >= 50:
            status = 'warning'
            interpretation = 'ì£¼ì˜ í•„ìš”'
        elif health_score >= 30:
            status = 'critical'
            interpretation = 'ì¦‰ì‹œ ì ê²€ í•„ìš”'
        else:
            status = 'failure'
            interpretation = 'ê³ ì¥ ê°€ëŠ¥ì„± ë†’ìŒ'

        # ì¶”ì„¸ ë¶„ì„
        trend_analysis = self.analyze_degradation_trend(measurements)

        return {
            'health_score': float(health_score),
            'status': status,
            'interpretation': interpretation,
            'mean': float(mean),
            'std_deviation': float(std),
            'target_value': target_value,
            'deviation': float(deviation),
            'trend': trend_analysis,
        }

    def analyze_degradation_trend(
        self,
        measurements: List[QualityMeasurement],
        window_size: int = 5
    ) -> Dict[str, Any]:
        """
        ì—´í™” ì¶”ì„¸ ë¶„ì„ (Degradation Trend)

        Args:
            measurements: ì¸¡ì •ê°’ ë¦¬ìŠ¤íŠ¸
            window_size: ì´ë™í‰ê·  ìœˆë„

        Returns:
            ì—´í™” ì¶”ì„¸ ë¶„ì„
        """
        if len(measurements) < window_size * 2:
            return {
                'trend': 'unknown',
                'degradation_rate': 0,
                'remaining_useful_life': None,
            }

        values = [m.measurement_value for m in measurements]
        n = len(values)

        # ì´ë™í‰ê·  ê³„ì‚°
        moving_avg = []
        for i in range(window_size, n + 1):
            window = values[i - window_size:i]
            moving_avg.append(np.mean(window))

        # ì„ í˜• íšŒê·€ë¡œ ì—´í™”ìœ¨ ê³„ì‚°
        x = np.arange(len(moving_avg))
        y = np.array(moving_avg)

        coefficients = np.polyfit(x, y, 1)
        degradation_rate = coefficients[0]  # ê¸°ìš¸ê¸°

        # ì¶”ì„¸ íŒì •
        if abs(degradation_rate) < 0.001:
            trend = 'stable'
        elif degradation_rate > 0:
            trend = 'degrading'
        else:
            trend = 'improving'

        return {
            'trend': trend,
            'degradation_rate': float(degradation_rate),
            'r_squared': float(np.corrcoef(x, y)[0] ** 2) if len(x) > 1 else 0,
        }

    def predict_failure_time(
        self,
        measurements: List[QualityMeasurement],
        usl: float,
        lsl: float
    ) -> Dict[str, Any]:
        """
        ê³ ì¥ ì˜ˆì¸¡ ì‹œê°„ ê³„ì‚°

        Args:
            measurements: ì¸¡ì •ê°’ ë¦¬ìŠ¤íŠ¸
            usl: ìƒí•œ ê·œê²©
            lsl: í•˜í•œ ê·œê²©

        Returns:
            ê³ ì¥ ì˜ˆì¸¡ ê²°ê³¼
        """
        values = [m.measurement_value for m in measurements]

        if len(values) < 5:
            return {
                'predicted_failure': None,
                'confidence': 'low',
                'interpretation': 'ë°ì´í„° ë¶€ì¡±',
            }

        # ì„ í˜• ì¶”ì„¸ ì¶”ì •
        x = np.arange(len(values))
        y = np.array(values)

        coefficients = np.polyfit(x, y, 1)
        slope = coefficients[0]
        intercept = coefficients[1]

        # í˜„ì¬ê°’
        current_value = values[-1]

        # ìƒí•œ/í•˜í•œê¹Œì§€ì˜ ê±°ë¦¬
        distance_to_usl = usl - current_value if slope > 0 else float('inf')
        distance_to_lsl = current_value - lsl if slope < 0 else float('inf')

        # ì˜ˆì¸¡ ì‹œê°„ ê³„ì‚°
        steps_to_failure = None
        failure_type = None

        if slope > 0:
            # ìƒìŠ¹ ì¶”ì„¸ - USL ì´ˆê³¼ ì˜ˆìƒ
            if distance_to_usl > 0 and slope > 0.0001:
                steps_to_failure = distance_to_usl / slope
                failure_type = 'upper_spec_exceeded'
        elif slope < 0:
            # í•˜ë½ ì¶”ì„¸ - LSL ë¯¸ë‹¬ ì˜ˆìƒ
            if distance_to_lsl > 0 and slope < -0.0001:
                steps_to_failure = distance_to_lsl / abs(slope)
                failure_type = 'lower_spec_exceeded'

        # ì‹ ë¢°ë„ ê³„ì‚°
        y_pred = np.polyval(coefficients, x)
        r_squared = 1 - np.sum((y - y_pred) ** 2) / np.sum((y - np.mean(y)) ** 2)

        confidence = 'high' if r_squared > 0.7 else 'medium' if r_squared > 0.4 else 'low'

        return {
            'predicted_failure_steps': int(steps_to_failure) if steps_to_failure is not None else None,
            'failure_type': failure_type,
            'confidence': confidence,
            'r_squared': float(r_squared),
            'trend_slope': float(slope),
            'interpretation': self._interpret_failure_prediction(steps_to_failure, failure_type, confidence),
        }

    def _interpret_failure_prediction(
        self,
        steps: Optional[float],
        failure_type: Optional[str],
        confidence: str
    ) -> str:
        """ê³ ì¥ ì˜ˆì¸¡ ê²°ê³¼ í•´ì„"""
        if steps is None:
            return "ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê·œê²© ì´íƒˆ ì—†ìŒ (ì•ˆì •)"

        if confidence == 'low':
            return f"ì˜ˆì¸¡ ì‹ ë¢°ë„ ë‚®ìŒ ({steps:.0f} ì¸¡ì • í›„ {failure_type} ê°€ëŠ¥ì„±)"

        if steps > 50:
            return f"ì•ˆì • ({steps:.0f} ì¸¡ì • í›„ {failure_type} ì˜ˆìƒ)"
        elif steps > 20:
            return f"ì£¼ì˜ í•„ìš” ({steps:.0f} ì¸¡ì • í›„ {failure_type} ì˜ˆìƒ)"
        elif steps > 10:
            return f"ì¡°ê¸° ì ê²€ ê¶Œì¥ ({steps:.0f} ì¸¡ì • í›„ {failure_type} ì˜ˆìƒ)"
        else:
            return f"ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš” ({steps:.0f} ì¸¡ì • í›„ {failure_type} ì˜ˆìƒ)"


class TimeSeriesService:
    """ì‹œê³„ì—´ ë¶„ì„ ì„œë¹„ìŠ¤ (ë©”ì¸ ì¸í„°í˜ì´ìŠ¤)"""

    def __init__(self):
        self.analyzer = TimeSeriesAnalyzer()
        self.forecast_engine = ForecastEngine()
        self.anomaly_detector = AnomalyDetector()
        self.predictive_maintenance = PredictiveMaintenance()

    def analyze_product_timeseries(
        self,
        product_id: int,
        days: int = 30
    ) -> Dict[str, Any]:
        """
        ì œí’ˆë³„ ì‹œê³„ì—´ ë¶„ì„

        Args:
            product_id: ì œí’ˆ ID
            days: ë¶„ì„ ê¸°ê°„ (ì¼)

        Returns:
            ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        from datetime import timedelta

        start_date = datetime.now() - timedelta(days=days)

        measurements = QualityMeasurement.objects.filter(
            product_id=product_id,
            measured_at__gte=start_date
        ).order_by('measured_at')

        if measurements.count() < 10:
            return {
                'error': 'ë°ì´í„° ë¶€ì¡±',
                'message': f'ìµœì†Œ {10}ê°œ ì´ìƒì˜ ì¸¡ì • ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.',
                'available_data': measurements.count(),
            }

        values = list(measurements.values_list('measurement_value', flat=True))
        timestamps = list(measurements.values_list('measured_at', flat=True))

        # ì¶”ì„¸ ë¶„ì„
        trend_analysis = self.analyzer.analyze_trend(values, timestamps)

        # ê³„ì ˆì„± ë¶„ì„
        seasonality_analysis = self.analyzer.detect_seasonality(values)

        # ì‹œê³„ì—´ ë¶„í•´
        decomposition = self.analyzer.decompose(values)

        # ì˜ˆì¸¡
        forecast = self.forecast_engine.combined_forecast(values, forecast_steps=5)

        # ì´ìƒ ê°ì§€
        anomalies = self.anomaly_detector.detect_statistical_anomalies(measurements)
        pattern_anomalies = self.anomaly_detector.detect_pattern_anomalies(measurements)

        return {
            'product_id': product_id,
            'analysis_period': f'{days} days',
            'data_points': len(values),
            'statistics': {
                'mean': float(np.mean(values)),
                'std_dev': float(np.std(values)),
                'min': float(np.min(values)),
                'max': float(np.max(values)),
                'cv': float(np.std(values) / np.mean(values)) if np.mean(values) != 0 else 0,
            },
            'trend_analysis': trend_analysis,
            'seasonality': seasonality_analysis,
            'decomposition': decomposition,
            'forecast': forecast,
            'anomalies': {
                'statistical': anomalies,
                'pattern': pattern_anomalies,
                'total_count': len(anomalies) + len(pattern_anomalies),
            },
        }

    def get_maintenance_prediction(
        self,
        product_id: int
    ) -> Dict[str, Any]:
        """
        ìœ ì§€ë³´ìˆ˜ ì˜ˆì¸¡

        Args:
            product_id: ì œí’ˆ ID

        Returns:
            ìœ ì§€ë³´ìˆ˜ ì˜ˆì¸¡ ê²°ê³¼
        """
        from datetime import timedelta

        # ì œí’ˆ ì •ë³´
        try:
            product = Product.objects.get(id=product_id)
        except Product.DoesNotExist:
            return {
                'error': 'Product not found',
                'product_id': product_id,
            }

        # ìµœê·¼ ë°ì´í„°
        start_date = datetime.now() - timedelta(days=30)
        measurements = QualityMeasurement.objects.filter(
            product_id=product_id,
            measured_at__gte=start_date
        ).order_by('measured_at')

        if measurements.count() < 10:
            return {
                'error': 'ë°ì´í„° ë¶€ì¡±',
                'message': 'ìµœì†Œ 30ì¼ê°„ 10ê°œ ì´ìƒì˜ ì¸¡ì • ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.',
                'available_data': measurements.count(),
            }

        # ê±´ì „ì„± ë¶„ì„
        health = self.predictive_maintenance.calculate_equipment_health(
            measurements,
            product.target_value,
            (product.usl - product.lsl) / 2  # ê·œê²© í­ì˜ ì ˆë°˜ì„ toleranceë¡œ ì‚¬ìš©
        )

        # ê³ ì¥ ì˜ˆì¸¡
        failure_prediction = self.predictive_maintenance.predict_failure_time(
            measurements,
            product.usl,
            product.lsl
        )

        # ì´ìƒ ê°ì§€
        anomalies = self.anomaly_detector.detect_statistical_anomalies(measurements)

        return {
            'product_id': product_id,
            'product_code': product.product_code,
            'product_name': product.product_name,
            'analysis_date': datetime.now().isoformat(),
            'data_points': measurements.count(),
            'health_status': health,
            'failure_prediction': failure_prediction,
            'recent_anomalies': anomalies[:10],  # ìµœê·¼ 10ê°œ
            'recommendations': self._generate_maintenance_recommendations(health, failure_prediction),
        }

    def _generate_maintenance_recommendations(
        self,
        health: Dict[str, Any],
        failure_prediction: Dict[str, Any]
    ) -> List[str]:
        """ìœ ì§€ë³´ìˆ˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        health_score = health.get('health_score', 100)
        status = health.get('status', '')

        # ê±´ì „ì„± ì ìˆ˜ ê¸°ë°˜ ê¶Œì¥
        if health_score < 30:
            recommendations.append("ğŸ”´ ê¸´ê¸‰: ì„¤ë¹„ ì¦‰ì‹œ ì •ì§€ ë° ì ê²€ í•„ìš”")
            recommendations.append("ğŸ”´ ì „ë¬¸ê°€ ìƒë‹´ ê¶Œì¥")
        elif health_score < 50:
            recommendations.append("ğŸŸ¡ ì£¼ì˜: ë‹¤ìŒ ì •ê¸° ì ê²€ ì‹œ ì„¸ë°€íˆ í™•ì¸ í•„ìš”")
            recommendations.append("ğŸŸ¡ ì¸¡ì • ë¹ˆë„ ì¦ê°€ ê¶Œì¥")
        elif health_score < 70:
            recommendations.append("ğŸŸ¢ ì •ìƒ: ì •ê¸° ì ê²€ ìœ ì§€")
        else:
            recommendations.append("âœ… ì–‘í˜¸: í˜„ì¬ ìƒíƒœ ìœ ì§€")

        # ê³ ì¥ ì˜ˆì¸¡ ê¸°ë°˜ ê¶Œì¥
        steps = failure_prediction.get('predicted_failure_steps')
        if steps is not None and steps < 20:
            recommendations.append(f"âš ï¸ ì˜ˆì¸¡ëœ {steps} ì¸¡ì • ë‚´ ê·œê²© ì´íƒˆ ê°€ëŠ¥ì„±")

        return recommendations
