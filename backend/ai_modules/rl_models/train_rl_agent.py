"""
PPO ì—ì´ì „íŠ¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
APS ìŠ¤ì¼€ì¤„ë§ ìµœì í™”ë¥¼ ìœ„í•œ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í›ˆë ¨
"""
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
import numpy as np
from pathlib import Path
import os

from aps_rl_env import APSSchedulingEnv

def train_ppo_agent(
    n_jobs=20,
    n_machines=5,
    total_timesteps=500000,
    save_dir='saved_models',
    use_parallel_envs=True,
    n_envs=4
):
    """
    PPO ì—ì´ì „íŠ¸ í•™ìŠµ

    Args:
        n_jobs: ì‘ì—… ìˆ˜
        n_machines: ì„¤ë¹„ ìˆ˜
        total_timesteps: ì´ í•™ìŠµ ìŠ¤í… ìˆ˜
        save_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        use_parallel_envs: ë³‘ë ¬ í™˜ê²½ ì‚¬ìš© ì—¬ë¶€
        n_envs: ë³‘ë ¬ í™˜ê²½ ìˆ˜
    """
    print("=" * 80)
    print("ğŸš€ PPO ì—ì´ì „íŠ¸ í•™ìŠµ ì‹œì‘")
    print("=" * 80)

    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    save_path = Path(__file__).parent / save_dir
    save_path.mkdir(parents=True, exist_ok=True)

    # í™˜ê²½ ì„¤ì •
    env_config = {
        'n_machines': n_machines,
        'n_jobs': n_jobs,
        'max_process_time': 100,
        'max_due_date': 500,
        'max_priority': 10,
        'tardiness_penalty': 2.0,
        'makespan_penalty': 0.5,
        'balance_reward': 10.0
    }

    # í™˜ê²½ ìƒì„±
    if use_parallel_envs:
        print(f"\nğŸ”§ ë³‘ë ¬ í™˜ê²½ ìƒì„± ({n_envs}ê°œ)...")
        env = make_vec_env(
            lambda: APSSchedulingEnv(config=env_config),
            n_envs=n_envs,
            vec_env_cls=SubprocVecEnv
        )
    else:
        print(f"\nğŸ”§ ë‹¨ì¼ í™˜ê²½ ìƒì„±...")
        env = APSSchedulingEnv(config=env_config)
        env = DummyVecEnv([lambda: env])

    # í‰ê°€ í™˜ê²½ (ë³„ë„)
    eval_env = APSSchedulingEnv(config=env_config)
    eval_env = DummyVecEnv([lambda: eval_env])

    # PPO í•˜ì´í¼íŒŒë¼ë¯¸í„°
    ppo_config = {
        'policy': 'MlpPolicy',
        'env': env,
        'learning_rate': 3e-4,
        'n_steps': 2048,
        'batch_size': 64,
        'n_epochs': 10,
        'gamma': 0.99,
        'gae_lambda': 0.95,
        'clip_range': 0.2,
        'ent_coef': 0.01,
        'vf_coef': 0.5,
        'max_grad_norm': 0.5,
        'verbose': 1,
        'tensorboard_log': str(save_path / 'tensorboard')
    }

    print(f"\nâš™ï¸ PPO í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    for key, value in ppo_config.items():
        if key not in ['policy', 'env']:
            print(f"   {key}: {value}")

    # PPO ì—ì´ì „íŠ¸ ìƒì„±
    print(f"\nğŸ¤– PPO ì—ì´ì „íŠ¸ ìƒì„± ì¤‘...")
    model = PPO(**ppo_config)

    # ì½œë°± ì„¤ì •
    # 1. í‰ê°€ ì½œë°± (10,000 ìŠ¤í…ë§ˆë‹¤ í‰ê°€)
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=str(save_path / 'best_model'),
        log_path=str(save_path / 'eval_logs'),
        eval_freq=10000,
        n_eval_episodes=10,
        deterministic=True,
        render=False,
        verbose=1
    )

    # 2. ì²´í¬í¬ì¸íŠ¸ ì½œë°± (50,000 ìŠ¤í…ë§ˆë‹¤ ì €ì¥)
    checkpoint_callback = CheckpointCallback(
        save_freq=50000,
        save_path=str(save_path / 'checkpoints'),
        name_prefix='ppo_aps_scheduling'
    )

    # í•™ìŠµ ì‹œì‘
    print(f"\nğŸ“ í•™ìŠµ ì‹œì‘ (Total timesteps: {total_timesteps:,})...")
    print(f"   TensorBoard ë¡œê·¸: {save_path / 'tensorboard'}")
    print(f"   Best ëª¨ë¸ ì €ì¥: {save_path / 'best_model'}")
    print(f"   ì²´í¬í¬ì¸íŠ¸: {save_path / 'checkpoints'}")

    model.learn(
        total_timesteps=total_timesteps,
        callback=[eval_callback, checkpoint_callback],
        progress_bar=True
    )

    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = save_path / 'final_model' / 'ppo_aps_scheduling'
    model.save(str(final_model_path))
    print(f"\nâœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_model_path}")

    # í™˜ê²½ ì¢…ë£Œ
    env.close()
    eval_env.close()

    print("\n" + "=" * 80)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("=" * 80)

    return model, save_path

def evaluate_agent(model_path, n_episodes=100, render=False):
    """
    í•™ìŠµëœ ì—ì´ì „íŠ¸ í‰ê°€

    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ
        n_episodes: í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜
        render: ë Œë”ë§ ì—¬ë¶€

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print("\n" + "=" * 80)
    print(f"ğŸ“Š ì—ì´ì „íŠ¸ í‰ê°€ ({n_episodes} ì—í”¼ì†Œë“œ)")
    print("=" * 80)

    # ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
    model = PPO.load(model_path)

    # í‰ê°€ í™˜ê²½ ìƒì„±
    env_config = {
        'n_machines': 5,
        'n_jobs': 20,
        'max_process_time': 100,
        'max_due_date': 500
    }
    env = APSSchedulingEnv(config=env_config)

    # í‰ê°€ ì‹¤í–‰
    episode_rewards = []
    episode_tardiness = []
    episode_makespans = []

    for ep in range(n_episodes):
        observation, info = env.reset()
        episode_reward = 0
        terminated = False
        truncated = False

        while not (terminated or truncated):
            action, _states = model.predict(observation, deterministic=True)
            observation, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward

        episode_rewards.append(episode_reward)
        episode_tardiness.append(info['total_tardiness'])
        episode_makespans.append(info['makespan'])

        if render and ep < 3:  # ì²˜ìŒ 3ê°œ ì—í”¼ì†Œë“œë§Œ ë Œë”ë§
            env.render()

    # ê²°ê³¼ í†µê³„
    results = {
        'mean_reward': np.mean(episode_rewards),
        'std_reward': np.std(episode_rewards),
        'mean_tardiness': np.mean(episode_tardiness),
        'std_tardiness': np.std(episode_tardiness),
        'mean_makespan': np.mean(episode_makespans),
        'std_makespan': np.std(episode_makespans),
        'min_tardiness': np.min(episode_tardiness),
        'max_tardiness': np.max(episode_tardiness)
    }

    print(f"\nğŸ“ˆ í‰ê°€ ê²°ê³¼:")
    print(f"   í‰ê·  ë³´ìƒ: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
    print(f"   í‰ê·  Tardiness: {results['mean_tardiness']:.2f} Â± {results['std_tardiness']:.2f}")
    print(f"   í‰ê·  Makespan: {results['mean_makespan']:.2f} Â± {results['std_makespan']:.2f}")
    print(f"   ìµœì†Œ Tardiness: {results['min_tardiness']:.2f}")
    print(f"   ìµœëŒ€ Tardiness: {results['max_tardiness']:.2f}")

    env.close()

    return results

def compare_with_baseline():
    """
    RL ì—ì´ì „íŠ¸ vs ë² ì´ìŠ¤ë¼ì¸ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ
    """
    print("\n" + "=" * 80)
    print("âš”ï¸  RL vs ë² ì´ìŠ¤ë¼ì¸ ë¹„êµ")
    print("=" * 80)

    # í™˜ê²½ ì„¤ì •
    env_config = {
        'n_machines': 5,
        'n_jobs': 20,
        'max_process_time': 100,
        'max_due_date': 500
    }

    # 1. RL ì—ì´ì „íŠ¸
    print("\n1ï¸âƒ£ RL ì—ì´ì „íŠ¸ (PPO)")
    model_path = Path(__file__).parent / 'saved_models' / 'best_model' / 'best_model.zip'
    if model_path.exists():
        rl_results = evaluate_agent(str(model_path), n_episodes=50)
    else:
        print("   âš ï¸  í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € train_ppo_agent()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    # 2. ë² ì´ìŠ¤ë¼ì¸: FIFO (First In First Out)
    print("\n2ï¸âƒ£ ë² ì´ìŠ¤ë¼ì¸: FIFO")
    fifo_results = evaluate_baseline('FIFO', env_config, n_episodes=50)

    # 3. ë² ì´ìŠ¤ë¼ì¸: SPT (Shortest Processing Time)
    print("\n3ï¸âƒ£ ë² ì´ìŠ¤ë¼ì¸: SPT")
    spt_results = evaluate_baseline('SPT', env_config, n_episodes=50)

    # 4. ë² ì´ìŠ¤ë¼ì¸: EDD (Earliest Due Date)
    print("\n4ï¸âƒ£ ë² ì´ìŠ¤ë¼ì¸: EDD")
    edd_results = evaluate_baseline('EDD', env_config, n_episodes=50)

    # ë¹„êµ ê²°ê³¼
    print("\n" + "=" * 80)
    print("ğŸ“Š ë¹„êµ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)

    comparison = {
        'RL (PPO)': rl_results,
        'FIFO': fifo_results,
        'SPT': spt_results,
        'EDD': edd_results
    }

    print(f"\n{'ì•Œê³ ë¦¬ì¦˜':<15} {'í‰ê·  Tardiness':>20} {'í‰ê·  Makespan':>20}")
    print("-" * 60)
    for name, result in comparison.items():
        print(f"{name:<15} {result['mean_tardiness']:>15.2f} Â± {result['std_tardiness']:<7.2f} {result['mean_makespan']:>15.2f} Â± {result['std_makespan']:<7.2f}")

    # ê°œì„ ìœ¨ ê³„ì‚°
    best_baseline_tardiness = min(fifo_results['mean_tardiness'], spt_results['mean_tardiness'], edd_results['mean_tardiness'])
    improvement = ((best_baseline_tardiness - rl_results['mean_tardiness']) / best_baseline_tardiness) * 100

    print(f"\nğŸ¯ RL ì—ì´ì „íŠ¸ ê°œì„ ìœ¨: {improvement:.2f}% (Best Baseline ëŒ€ë¹„)")

def evaluate_baseline(rule, env_config, n_episodes=50):
    """
    ë² ì´ìŠ¤ë¼ì¸ dispatch rule í‰ê°€
    """
    env = APSSchedulingEnv(config=env_config)

    episode_tardiness = []
    episode_makespans = []

    for ep in range(n_episodes):
        observation, info = env.reset()
        terminated = False
        truncated = False

        while not (terminated or truncated):
            # ê·œì¹™ ê¸°ë°˜ ì•¡ì…˜ ì„ íƒ
            action = select_action_by_rule(env, rule)
            observation, reward, terminated, truncated, info = env.step(action)

        episode_tardiness.append(info['total_tardiness'])
        episode_makespans.append(info['makespan'])

    results = {
        'mean_tardiness': np.mean(episode_tardiness),
        'std_tardiness': np.std(episode_tardiness),
        'mean_makespan': np.mean(episode_makespans),
        'std_makespan': np.std(episode_makespans)
    }

    print(f"   í‰ê·  Tardiness: {results['mean_tardiness']:.2f} Â± {results['std_tardiness']:.2f}")
    print(f"   í‰ê·  Makespan: {results['mean_makespan']:.2f} Â± {results['std_makespan']:.2f}")

    env.close()
    return results

def select_action_by_rule(env, rule):
    """
    ê·œì¹™ ê¸°ë°˜ ì•¡ì…˜ ì„ íƒ
    """
    # ë¯¸ìŠ¤ì¼€ì¤„ ì‘ì—… ì°¾ê¸°
    unscheduled_jobs = [i for i in range(env.n_jobs) if not env.scheduled_jobs[i]]

    if not unscheduled_jobs:
        return 0  # ë” ì´ìƒ ì‘ì—… ì—†ìŒ

    # ê·œì¹™ì— ë”°ë¼ ì‘ì—… ì„ íƒ
    if rule == 'FIFO':
        selected_job = unscheduled_jobs[0]
    elif rule == 'SPT':
        selected_job = min(unscheduled_jobs, key=lambda j: env.jobs[j]['process_time'])
    elif rule == 'EDD':
        selected_job = min(unscheduled_jobs, key=lambda j: env.jobs[j]['due_date'])
    else:
        selected_job = unscheduled_jobs[0]

    # ê°€ìš©í•œ ì„¤ë¹„ ì„ íƒ (ê°€ì¥ ë¹¨ë¦¬ ëë‚˜ëŠ” ì„¤ë¹„)
    eligible_machines = [m for m in range(env.n_machines) if env.jobs[selected_job]['machine_eligibility'][m]]
    selected_machine = min(eligible_machines, key=lambda m: env.machine_available_times[m])

    # ì•¡ì…˜ ì¸ì½”ë”©
    action = selected_job * env.n_machines + selected_machine

    return action

if __name__ == '__main__':
    import sys

    if len(sys.argv) > 1:
        command = sys.argv[1]

        if command == 'train':
            # í•™ìŠµ ì‹¤í–‰
            train_ppo_agent(
                n_jobs=20,
                n_machines=5,
                total_timesteps=500000,
                use_parallel_envs=True,
                n_envs=4
            )

        elif command == 'eval':
            # í‰ê°€ ì‹¤í–‰
            model_path = Path(__file__).parent / 'saved_models' / 'best_model' / 'best_model.zip'
            evaluate_agent(str(model_path), n_episodes=100, render=True)

        elif command == 'compare':
            # ë¹„êµ ì‹¤í–‰
            compare_with_baseline()

        else:
            print(f"Unknown command: {command}")
            print("Usage: python train_rl_agent.py [train|eval|compare]")

    else:
        print("Usage: python train_rl_agent.py [train|eval|compare]")
        print("\nì˜ˆì‹œ:")
        print("  python train_rl_agent.py train      # ì—ì´ì „íŠ¸ í•™ìŠµ")
        print("  python train_rl_agent.py eval       # ì—ì´ì „íŠ¸ í‰ê°€")
        print("  python train_rl_agent.py compare    # ë² ì´ìŠ¤ë¼ì¸ê³¼ ë¹„êµ")
